# GPU

### TODO:

1. GPU:
    1. https://www.androidauthority.com/what-is-a-gpu-gary-explains-693542/
    3. https://blogs.nvidia.com/blog/2018/06/11/what-is-a-virtual-gpu/
    5. [DGX](https://devblogs.nvidia.com/dgx-1-fastest-deep-learning-system/)
   
2. ML <3 GPUs
    1. https://www.youtube.com/watch?v=6stDhEA0wFQ
    3. [Pytorch parallel](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)
    4. [Rapids](https://towardsdatascience.com/how-to-use-gpus-for-machine-learning-with-the-new-nvidia-data-science-workstation-64ef37460fa0)

### Key Concepts:
- GPU vs CPU
- ALU
- GDDR
- CUDA
- DGX
- VGPU
- Rapids
- NGC
- Sharding

### Key Questions:
- What is the primary difference between a GPU and a CPU in terms of architecture and use case?
- What role does the ALU (Arithmetic Logic Unit) play in a CPU or GPU? How does it differ in scale between the two?
- What is GDDR memory and how does it differ from the DDR memory found in standard CPUs? Why is GDDR preferred in GPUs?
- What is CUDA and why is it significant in the context of machine learning or data science?
- Give a real-world example of a task that can be significantly accelerated using CUDA-enabled GPUs.
- What is a VGPU (Virtual GPU)? How might it be useful in a cloud-based ML training environment?
- What is a DGX system, and what type of workloads is it designed to support?
- What is NGC (NVIDIA GPU Cloud), and how might an MLOps engineer use it in a project?
- What is NVIDIA RAPIDS, and how does it compare to using pandas and scikit-learn on a CPU?
- Explain what sharding means in a distributed GPU training context. How does it help scale training across multiple GPUs or nodes?
- In a multi-GPU training setup, what are some challenges sharding introduces, and how can tools like NVIDIA NCCL help mitigate them?

